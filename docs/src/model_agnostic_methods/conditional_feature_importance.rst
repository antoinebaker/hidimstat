.. _conditional_feature_importance:


==============================
Conditional Feature Importance
==============================

Theoretical index
------------------


Conditional Feature Importance (CFI) is a model-agnostic method that estimates feature 
importance using perturbations. It generates a perturbed feature :math:`X_j^P` that is
sampled conditionally on the other features :math:`X_j^P \sim P(X_j | X_{-j})`. The
predictive model is then evaluated on the perturbed feature vector :math:`\tilde X = \left[X_1, ...,X_j^P, ..., X_p\right]`,
and the feature importance is defined as the performance drop of the model on the 
perturbed data:

.. math::
    \psi_j^{CFI} = \mathbb{E} \left[\mathcal{L}\left(y, \mu(\tilde X)\right)\right] - \mathbb{E} \left[\mathcal{L}\left(y, \mu(X)\right)\right],


The target quantity estimated by CFI is the Total Sobol Index (TSI) :ref:`total_sobol_index`. 
Indeed, 

.. math::
    \frac{1}{2} \psi_j^{CFI} &= \mathbb{E} \left[\mathcal{L}\left(y, \mu_{-j}(X^-j)\right)\right] - \mathbb{E} \left[\mathcal{L}\left(y, \mu(X)\right)\right] \\
    &= \psi_j^{TSI}.


Estimation procedure
--------------------

The estimation of CFI relies on the ability to sample the perturbed feature matrix 
:math:`\tilde X` and more specifically to sample :math:`X_j^p` from the conditional 
distribution, :math:`X_j^p \sim P(X_j | X_{-j})`. This can be achieved using the 
conditional permutation approach (:footcite:t:`Chamma_NeurIPS2023`). The procedure relies on the 
decomposition of the :math:`j^{th}` feature into a part that is predictable from the
other features, and an "additive innovation" or residual information that is independent
of the other features:

.. math::
    X_j = \nu_j(X_{-j}) + \epsilon_j, \quad \text{with} \quad \epsilon_j \perp\!\!\!\perp X_{-j} \text{ and } \mathbb{E}[\epsilon_j] = 0.

Here :math:`\nu_j(X_{-j}) = \mathbb{E}[X_j | X_{-j}]` is the conditional expectation of
:math:`X_j` given the other features. In practice, :math:`\nu_j` is unknown and has to be
estimated from the data using a predictive model. 

Then the perturbed feature :math:`X_j^p` is generated by keeping the predictable part
:math:`\nu_j(X_{-j})` unchanged, and by replacing the residual :math:`\epsilon_j` by a
randomly permuted version :math:`\epsilon_j^p`:

.. math::
    X_j^p = \nu_j(X_{-j}) + \epsilon_j^p, \quad \text{with} \quad \epsilon_j^p \sim \text{Perm}(\epsilon_j).


.. note:: **Estimation of :math:`X_j^p`**

    TODO: note on the fact estimating :math:`\nu_j` is supposed to be an easy task. 
    Note on the fact that we can use any valid conditional sampler can be used.

Inference
---------
TODO finish the section on inference

References
----------
.. footbibliography::
