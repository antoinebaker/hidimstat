.. _conditional_feature_importance:


==============================
Conditional Feature Importance
==============================

Theoretical index
------------------


Conditional Feature Importance (CFI) is a model-agnostic method that estimates
feature importance using perturbations. It generates a perturbed feature
:math:`X_j^P` that is sampled conditionally on the other features :math:`X_j^P
\sim P(X_j | X_{-j})`. The predictive model is then evaluated on the perturbed
feature vector :math:`\tilde X = \left[X_1, ...,X_j^P, ..., X_p\right]`, and the
feature importance is defined as the performance drop of the model on the
perturbed data:

.. math::
    \psi_j^{CFI} = \mathbb{E} [\mathcal{L}(y, \mu(\tilde X))] - \mathbb{E} [\mathcal{L}(y, \mu(X))].


The target quantity estimated by CFI is the Total Sobol Index (TSI) :ref:`total_sobol_index`. 
Indeed, 

.. math::
    \frac{1}{2} \psi_j^{CFI} 
    = \psi_j^{TSI} 
    = \mathbb{E} [\mathcal{L}(y, \mu_{-j}(X^-j))] - \mathbb{E} [\mathcal{L}(y, \mu(X))].


Estimation procedure
--------------------

The estimation of CFI relies on the ability to sample the perturbed feature matrix 
:math:`\tilde X` and more specifically to sample :math:`X_j^p` from the conditional 
distribution, :math:`X_j^p \sim P(X_j | X_{-j})`. This can be achieved using the 
conditional permutation approach (:footcite:t:`Chamma_NeurIPS2023`). The procedure relies on the 
decomposition of the :math:`j^{th}` feature into a part that is predictable from the
other features, and an "additive innovation" or residual information that is independent
of the other features:

.. math::
    X_j = \nu_j(X_{-j}) + \epsilon_j, \quad \text{with} \quad \epsilon_j \perp\!\!\!\perp X_{-j} \text{ and } \mathbb{E}[\epsilon_j] = 0.

Here :math:`\nu_j(X_{-j}) = \mathbb{E}[X_j | X_{-j}]` is the conditional expectation of
:math:`X_j` given the other features. In practice, :math:`\nu_j` is unknown and has to be
estimated from the data using a predictive model. 

Then the perturbed feature :math:`X_j^p` is generated by keeping the predictable part
:math:`\nu_j(X_{-j})` unchanged, and by replacing the residual :math:`\epsilon_j` by a
randomly permuted version :math:`\epsilon_j^p`:

.. math::
    X_j^p = \nu_j(X_{-j}) + \epsilon_j^p, \quad \text{with} \quad \epsilon_j^p \sim \text{Perm}(\epsilon_j).


.. note:: **Estimation of :math:`X_j^p`**

    TODO: note on the fact estimating :math:`\nu_j` is supposed to be an easy task. 
    Note on the fact that we can use any valid conditional sampler can be used.


Inference
---------
TODO finish the section on inference


Regression example
------------------
The following example illustrates the use of CFI on a regression task with::

    >>> from sklearn.datasets import make_regression
    >>> from sklearn.linear_model import LinearRegression
    >>> from sklearn.model_selection import train_test_split
    >>> from hidimstat import CFI


    >>> X, y = make_regression(n_features=2)
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y)
    >>> model = LinearRegression().fit(X_train, y_train)
    
    >>> cfi = CFI(estimator=model, imputation_model_continuous=LinearRegression())
    >>> cfi.fit(X_train, y_train)
    >>> features_importance = cfi.importance(X_test, y_test)


Classification example
----------------------
To measure feature importance in a classification task, a classification loss should be
used, in addition, the prediction method of the estimator should output the corresponding 
type of prediction (probabilities or classes). The following example illustrates the use
of CFI on a classification task with::

    >>> from sklearn.datasets import make_classification
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.linear_model import LinearRegression
    >>> from sklearn.metrics import log_loss
    >>> from sklearn.model_selection import train_test_split
    >>> from hidimstat import CFI

    >>> X, y = make_classification(n_features=4)
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y)
    >>> model = RandomForestClassifier().fit(X_train, y_train)
    >>> cfi = CFI(
    ...     estimator=model,
    ...     imputation_model_continuous=LinearRegression(),
    ...     loss=log_loss,
    ...     method="predict_proba",
    ... )
    >>> cfi.fit(X_train, y_train)
    >>> features_importance = cfi.importance(X_test, y_test)

References
----------
.. footbibliography::
