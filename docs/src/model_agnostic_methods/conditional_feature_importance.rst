.. _conditional_feature_importance:


==============================
Conditional Feature Importance
==============================

Conditional Feature Importance (CFI) is a model-agnostic approach for quantifying the 
relevance of individual or groups of features in predictive models. Unlike methods such 
as :ref:`leave-one-covariate-out <leave_one_covariate_out>` (LOCO), CFI does not require 
retraining or refitting the predictive model for each feature, making it computationally 
efficient. 


.. image:: ../images/sphx_glr_plot_cfi_001.png


Theoretical index
------------------

Conditional Feature Importance (CFI) is a model-agnostic method that estimates
feature importance using perturbations. It generates a perturbed feature
:math:`X_j^P` that is sampled conditionally on the other features :math:`X_j^P
\sim P(X_j | X_{-j})`. The predictive model is then evaluated on the perturbed
feature vector :math:`\tilde X = \left[X_1, ...,X_j^P, ..., X_p\right]`, and the
feature importance is defined as the performance drop of the model :math:`\mu` on the
perturbed data:

.. math::
    \psi_j^{CFI} = \mathbb{E} [\mathcal{L}(y, \mu(\tilde X))] - \mathbb{E} [\mathcal{L}(y, \mu(X))].


The target quantity estimated by CFI is the Total Sobol Index (TSI) :ref:`total_sobol_index`. 
Indeed, 

.. math::
    \frac{1}{2} \psi_j^{CFI} 
    = \psi_j^{TSI} 
    = \mathbb{E} [\mathcal{L}(y, \mu_{-j}(X^-j))] - \mathbb{E} [\mathcal{L}(y, \mu(X))].

Where in regression, :math:`\mu_{-j}(X_{-j}) = \mathbb{E}[\mu(X) | X_{-j}]` is the 
model fitted without the :math:`j^{th}` feature.

Estimation procedure
--------------------

The estimation of CFI relies on the ability to sample the perturbed feature matrix 
:math:`\tilde X` and more specifically to sample :math:`X_j^p` from the conditional 
distribution, :math:`X_j^p \sim P(X_j | X_{-j})`. This can be achieved using the 
conditional permutation approach (:footcite:t:`Chamma_NeurIPS2023`). The procedure relies on the 
decomposition of the :math:`j^{th}` feature into a part that is predictable from the
other features, and a residual information term that is independent of the other features:

.. math::
    X_j = \nu_j(X_{-j}) + \epsilon_j, \quad \text{with} \quad \epsilon_j \perp\!\!\!\perp X_{-j} \text{ and } \mathbb{E}[\epsilon_j] = 0.

Here :math:`\nu_j(X_{-j}) = \mathbb{E}[X_j | X_{-j}]` is the conditional expectation of
:math:`X_j` given the other features. In practice, :math:`\nu_j` is unknown and has to be
estimated from the data using a predictive model. 

Then the perturbed feature :math:`X_j^p` is generated by keeping the predictable part
:math:`\nu_j(X_{-j})` unchanged, and by replacing the residual :math:`\epsilon_j` by a
randomly permuted version :math:`\epsilon_j^p`:

.. math::
    X_j^p = \nu_j(X_{-j}) + \epsilon_j^p, \quad \text{with} \quad \epsilon_j^p \sim \text{Perm}(\epsilon_j).


.. note:: **Estimation of** :math:`X_j^p`

    Estimating :math:`\nu_j` that is modeling the relationship between features is 
    arguably an easier task than estimating the relationship between features and the 
    target. This assumption was for instance argued in :footcite:t:`Chamma_NeurIPS2023`, 
    :footcite:t:`candes2018panning`. Consequently, simple predictive models such as 
    regularized linear models or decision trees can be used to estimate :math:`\nu_j`.


Inference
---------
TODO finish the section on inference


Regression example
------------------
The following example illustrates the use of CFI on a regression task with::

    >>> from sklearn.datasets import make_regression
    >>> from sklearn.linear_model import LinearRegression
    >>> from sklearn.model_selection import train_test_split
    >>> from hidimstat import CFI


    >>> X, y = make_regression(n_features=2)
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y)
    >>> model = LinearRegression().fit(X_train, y_train)
    
    >>> cfi = CFI(estimator=model, imputation_model_continuous=LinearRegression())
    >>> cfi = cfi.fit(X_train, y_train)
    >>> features_importance = cfi.importance(X_test, y_test)


Classification example
----------------------
To measure feature importance in a classification task, a classification loss should be
used, in addition, the prediction method of the estimator should output the corresponding 
type of prediction (probabilities or classes). The following example illustrates the use
of CFI on a classification task with::

    >>> from sklearn.datasets import make_classification
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.linear_model import LinearRegression
    >>> from sklearn.metrics import log_loss
    >>> from sklearn.model_selection import train_test_split
    >>> from hidimstat import CFI

    >>> X, y = make_classification(n_features=4)
    >>> X_train, X_test, y_train, y_test = train_test_split(X, y)
    >>> model = RandomForestClassifier().fit(X_train, y_train)
    >>> cfi = CFI(
    ...     estimator=model,
    ...     imputation_model_continuous=LinearRegression(),
    ...     loss=log_loss,
    ...     method="predict_proba",
    ... )
    >>> cfi = cfi.fit(X_train, y_train)
    >>> features_importance = cfi.importance(X_test, y_test)

References
----------
.. footbibliography::
